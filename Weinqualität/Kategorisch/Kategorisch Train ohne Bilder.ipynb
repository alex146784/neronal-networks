{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren des neuronalen Netzes Kategorisch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Beginn müssen wieder alle Bibliotheken eingebunden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import IPython\n",
    "import kerastuner as kt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Fall, dass die bereits mit Skript 1 vorbereiteten Datensätze nicht im selben Verzeichnis wie die Python-Datei liegt muss das Verzeichnis des Datensatzes angegeben werden. Dies kann mittels folgendem Befehl durchgeführt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to set the start working directory manually\n",
    "#not necessary, if the .py file is executed in the directory of the .csv files\n",
    "#os.chdir('D:\\\\OneDrive - bwedu\\\\Uni\\\\09 ABC 1\\\\Neuronale Netzwerke\\\\Python\\\\Wein')\n",
    "\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun muss das Arbeitsverzeichnis geändert werden. \n",
    "Dabei wird das Arbeitsverzeichnis so geändert, dass auf die zuvor vorbereiteten Daten zugegriffen werden kann. \n",
    "\n",
    "Codeabsatz 1 speichert das aktuelle Arbeitsverzeichnis.\n",
    "\n",
    "Codeabsatz 2 ändert das Arbeitsverzeichnis.\n",
    "\n",
    "Codeabsatz 3 importiert die mittels Datenvorbereitungsskript zuvor erstellten Daten.\n",
    "\n",
    "Codeabsatz 4 convertiert die Daten in ein numpy-Array. \n",
    "\n",
    "Dabei ist zu beachten, dass die erste Spalte des Datensatzes nicht von Relevanz ist und daher nicht beachtet wird.\n",
    "\n",
    "Die Daten werden von Kategorie 3-9 in Kategorie 0-6 konvertiert.\n",
    "\n",
    "Im letzten Codeabsatz wird nach dem Import der Daten wieder in das Ausgangsverzeichnis gewechselt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the current working directory\n",
    "current_wd = os.getcwd()\n",
    "\n",
    "#change the working directory to the 'prep_dataset' directroy, where the prepared data a saved as .csv files\n",
    "os.chdir('prep_dataset')\n",
    "\n",
    "#import the prepared data as panda dataframes\n",
    "W_train_samples = pd.read_csv('wine_train_samples.csv')\n",
    "W_train_labels = pd.read_csv('wine_train_labels_cont.csv')\n",
    "\n",
    "#convert panda dataframe to a numpy array\n",
    "#leave out the first column, because this are the rownumernumbers\n",
    "W_train_samples = pd.DataFrame.to_numpy(W_train_samples)[:,1:13]\n",
    "W_train_labels = pd.DataFrame.to_numpy(W_train_labels)[:,1]-3\n",
    "\n",
    "#change the working directory back to the original working directory\n",
    "os.chdir(current_wd)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Codezeilen erzeugen ein zufälliges neuronales Netz (model_builder), welches später trainiert werden kann. \n",
    "\n",
    "Zu beachten ist, dass dem model_builder die Parameter (Anzahl layer, learning Rate und Anzahl Knoten) systematisch auswählt.\n",
    "\n",
    "\n",
    "Für dieses Beispiel kann der model_builder die Parameter in folgenden Bereichen wählen:\n",
    "\n",
    "Anzahl hidden layer: 1-5\n",
    "\n",
    "Learning Rate: 1e-5 bis 1e-4\n",
    "\n",
    "Knoten der hidden Layer: 16-160 (Differenz der Anzahl an Knoten zwischen den einzelnen Layer: 16 Knoten)\n",
    "\n",
    "Aktivierungsfunktion: relu bzw. softmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Beim kompilieren des Modells ist zu beachten, dass hier als Fehler (loss) die sparse categorial crossentropy berechnet wird. \n",
    "\n",
    "\n",
    "Zum Schluss wird das fertige Modell zurückgegeben. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the definition of the model building function\n",
    "#here it is possible to specify the model, the most parameters\n",
    "#with hp funcitions it is possible to specify a range of values, in which an optimizer can optimize the model\n",
    "def model_builder(hp):\n",
    "    #the number of layers in a range from 1 to 5\n",
    "\thp_layers = hp.Int('layers', min_value = 1, max_value = 5) \n",
    "    #test different learning rates\n",
    "\thp_learning_rate = hp.Choice('learning_rate', values = [1e-4, 3*1e-5, 1e-5]) \n",
    "\t\n",
    "\t#the input layer with 12 nodes\n",
    "\tinputs = keras.Input(shape=(12,))\n",
    "\t\n",
    "\tx = inputs\n",
    "\tfor i in range(hp_layers):\n",
    "\t\t#for each layer a different number of nodes (16 to 160)\n",
    "\t\tx = keras.layers.Dense(units = hp.Int('units_' + str(i), \n",
    "            min_value = 16, max_value = 160, step = 16), activation = \"relu\")(x)\n",
    "\t\n",
    "\t#the output layer with 7 nodes\n",
    "\toutputs = keras.layers.Dense(7, activation = \"softmax\")(x)\n",
    "\t\n",
    "\t#set the model together\n",
    "\tmodel = keras.Model(inputs, outputs)\n",
    "\t\n",
    "\t#compile the model\n",
    "\tmodel.compile(optimizer = keras.optimizers.Adam(learning_rate \n",
    "                  = hp_learning_rate), \n",
    "\t\t\t\t\tloss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "\t\t\t\t\tmetrics = ['accuracy'])\n",
    "\n",
    "\treturn model #return the finished model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die Daten im Weindatensatz (Qualität 3-9) nicht ausgeglichen sind, \n",
    "werden diese hier mit Gewichtungen versehen (class_weight), man beachte, dass Python hier die Zählung mit 0 beginnt (0-6 entspricht Güte 3-9).\n",
    "\n",
    "Im 2. Absatz werden die einzelnen Einstellungen für den tuner definiert. \n",
    "\n",
    "Wichtig: Sollte die Optimierung mehrmals wiederholt werden, so muss vor jeder Wiederholung zuerst das \n",
    "Verzeichnis des vorherigen tunings manuell gelöscht werden.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights to consider the circumstance, that the training data aren't distributed equally\n",
    "class_weight = {0: 9.4, 1: 1.3, 2: 0.13, 3: 0.1, 4: 0.26, 5: 1.47, 6: 56.7}\n",
    "\n",
    "#set settings for the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 400,\n",
    "                     \n",
    "                     factor = 3, #the higher, the faster the optimizing, but the smalles the probability to find the best model\n",
    "                     \n",
    "                            #how often should search the optimizer for optimal model\n",
    "\t\t\t\t\t\t\thyperband_iterations = 1, \n",
    "                             \n",
    "\t\t\t\t\t\t\t\n",
    "                     \n",
    "                     #!!!delete directory before optimizing again!!!\n",
    "                     directory = 'tuner_wine_cat_full', #where should the optimizer save the log files for the optimizing\n",
    "                     project_name = 'wine_quality_advanced')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Code verwenden wir sogenannte Callback-Funktionen. \n",
    "\n",
    "Sobald über 10 Epochen (patience) kein Fortschritt (Verbesserung der Genauigkeit) mehr erreicht wird, wird das Training abgebrochen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the necessary callbacks \n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden die erzeugten neuronalen Netze optimiert. \n",
    "Folgende Parameter werden dafür verwendet:\n",
    "    \n",
    "    epochs: 400 -> Insgesamt maximal 400 Wiederholungen\n",
    "    batch_size: 12 -> Paralleses Training des neuronalen Netzes durch Daten\n",
    "    validation_split: 0.1 -> 10% der Daten werden für eine Validation verwendet\n",
    "    callbacks (Abruf wie oben definiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 725 Complete [00h 02m 40s]\n",
      "val_accuracy: 0.3603448271751404\n",
      "\n",
      "Best val_accuracy So Far: 0.5189655423164368\n",
      "Total elapsed time: 01h 04m 08s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#start the optimizer \n",
    "tuner.search(W_train_samples, W_train_labels, epochs = 400, batch_size = 12, validation_split = 0.1, callbacks = [early_stopping_cb] ,class_weight = class_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden Befehl wird aus allen getesteten neuronalen Netzen das beste ausgewählt und gespeichert (alle anderen neuronalen Netze werden nicht beachtet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden print-Befehl werden die vorher definierten Parameter des besten neuronalen Netzes am Bildschirm ausgegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tThe hyperparameter search is complete.\n",
      "\t\tLearning rate: 0.0001\n",
      "\t\tNumber of Layers: 2\n",
      "\t\t\n",
      "Number of nodes in layer 0: 160.000000\n",
      "Number of nodes in layer 1: 112.000000\n"
     ]
    }
   ],
   "source": [
    "#print the results of the optimizer\n",
    "print(f\"\"\"\n",
    "\t\tThe hyperparameter search is complete.\n",
    "\t\tLearning rate: {best_hps.get('learning_rate')}\n",
    "\t\tNumber of Layers: {best_hps.get('layers')}\n",
    "\t\t\"\"\")\t\n",
    "for i in range(best_hps.get('layers')):\n",
    "\tprint(\"Number of nodes in layer %i: %f\" %(i, best_hps.get('units_' + str(i))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden Befehl wird die log file directory für die graphische Darstellung mittels tensorboard definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the log-directory for the logfiles for tensorboard\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "\timport time\n",
    "\trun_id = 'wine_quality_cat_full_' + time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "\treturn os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die callbacks werden wie folgt implementiert:\n",
    "\n",
    "Codezeile 1: Speichern des besten Resultats als \"wine_quality_cat_full4.h5\"\n",
    "\n",
    "Codezeile 2: Das neuronale Netz speichert immer nur das Modell mit der höchsten Genauigkeit als Variable in Python.\n",
    "\n",
    "Codezeile 3: Das neuronale Netz stoppt das Training, wenn nach 20 Wiederholungen keine weitere Erhöhung der Genauigkeit stattfindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the necessary callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"wine_quality_cat_full4.h5\", save_best_only=True)\n",
    "early_stopping_cb2 = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In folgendem Schritt wird das optimale Modell der Variable model zugewiesen, die nun das optimale untrainierte neuronale Netz repräsentiert, das anschließend noch trainiert werden muss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit folgendem Befehl erfolgt das finale Training des neuronalen Netzes.\n",
    "Mit der batch_size 6, maximal 1000 Epochen, 10% der Daten als Validierung und der callback-Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "   1/5220 [..............................] - ETA: 0s - loss: 0.2517 - accuracy: 0.0000e+00WARNING:tensorflow:From C:\\Users\\Tobia\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0326s). Check your callbacks.\n",
      "5220/5220 [==============================] - 4s 766us/step - loss: 0.5946 - accuracy: 0.3500 - val_loss: 1.8723 - val_accuracy: 0.3862\n",
      "Epoch 2/1000\n",
      "5220/5220 [==============================] - 4s 711us/step - loss: 0.5882 - accuracy: 0.2626 - val_loss: 1.8525 - val_accuracy: 0.3431\n",
      "Epoch 3/1000\n",
      "5220/5220 [==============================] - 4s 715us/step - loss: 0.5809 - accuracy: 0.2841 - val_loss: 1.8112 - val_accuracy: 0.3655\n",
      "Epoch 4/1000\n",
      "5220/5220 [==============================] - 4s 698us/step - loss: 0.5757 - accuracy: 0.2937 - val_loss: 1.8343 - val_accuracy: 0.2776\n",
      "Epoch 5/1000\n",
      "5220/5220 [==============================] - 4s 693us/step - loss: 0.5726 - accuracy: 0.2807 - val_loss: 1.7801 - val_accuracy: 0.3690\n",
      "Epoch 6/1000\n",
      "5220/5220 [==============================] - 4s 707us/step - loss: 0.5698 - accuracy: 0.3092 - val_loss: 1.8339 - val_accuracy: 0.2879\n",
      "Epoch 7/1000\n",
      "5220/5220 [==============================] - 4s 702us/step - loss: 0.5684 - accuracy: 0.3153 - val_loss: 1.7907 - val_accuracy: 0.3379\n",
      "Epoch 8/1000\n",
      "5220/5220 [==============================] - 4s 690us/step - loss: 0.5670 - accuracy: 0.3224 - val_loss: 1.7822 - val_accuracy: 0.3534\n",
      "Epoch 9/1000\n",
      "5220/5220 [==============================] - 4s 711us/step - loss: 0.5659 - accuracy: 0.3218 - val_loss: 1.7720 - val_accuracy: 0.3586\n",
      "Epoch 10/1000\n",
      "5220/5220 [==============================] - 4s 703us/step - loss: 0.5649 - accuracy: 0.3500 - val_loss: 1.8057 - val_accuracy: 0.3155\n",
      "Epoch 11/1000\n",
      "5220/5220 [==============================] - 4s 714us/step - loss: 0.5639 - accuracy: 0.3297 - val_loss: 1.7310 - val_accuracy: 0.4397\n",
      "Epoch 12/1000\n",
      "5220/5220 [==============================] - 4s 712us/step - loss: 0.5625 - accuracy: 0.3402 - val_loss: 1.7523 - val_accuracy: 0.4103\n",
      "Epoch 13/1000\n",
      "5220/5220 [==============================] - 4s 715us/step - loss: 0.5612 - accuracy: 0.3603 - val_loss: 1.8115 - val_accuracy: 0.3241\n",
      "Epoch 14/1000\n",
      "5220/5220 [==============================] - 4s 701us/step - loss: 0.5608 - accuracy: 0.3487 - val_loss: 1.8059 - val_accuracy: 0.3293\n",
      "Epoch 15/1000\n",
      "5220/5220 [==============================] - 4s 686us/step - loss: 0.5584 - accuracy: 0.3433 - val_loss: 1.7538 - val_accuracy: 0.4069\n",
      "Epoch 16/1000\n",
      "5220/5220 [==============================] - 4s 691us/step - loss: 0.5589 - accuracy: 0.3623 - val_loss: 1.8233 - val_accuracy: 0.3190\n",
      "Epoch 17/1000\n",
      "5220/5220 [==============================] - 4s 696us/step - loss: 0.5574 - accuracy: 0.3563 - val_loss: 1.7582 - val_accuracy: 0.4034\n",
      "Epoch 18/1000\n",
      "5220/5220 [==============================] - 4s 705us/step - loss: 0.5567 - accuracy: 0.3596 - val_loss: 1.7762 - val_accuracy: 0.3655\n",
      "Epoch 19/1000\n",
      "5220/5220 [==============================] - 4s 723us/step - loss: 0.5561 - accuracy: 0.3536 - val_loss: 1.7439 - val_accuracy: 0.4103\n",
      "Epoch 20/1000\n",
      "5220/5220 [==============================] - 4s 719us/step - loss: 0.5557 - accuracy: 0.3458 - val_loss: 1.7359 - val_accuracy: 0.4276\n",
      "Epoch 21/1000\n",
      "5220/5220 [==============================] - 4s 700us/step - loss: 0.5544 - accuracy: 0.3527 - val_loss: 1.7355 - val_accuracy: 0.4431\n",
      "Epoch 22/1000\n",
      "5220/5220 [==============================] - 4s 699us/step - loss: 0.5543 - accuracy: 0.3603 - val_loss: 1.7638 - val_accuracy: 0.3948\n",
      "Epoch 23/1000\n",
      "5220/5220 [==============================] - 4s 698us/step - loss: 0.5533 - accuracy: 0.3690 - val_loss: 1.7701 - val_accuracy: 0.3931\n",
      "Epoch 24/1000\n",
      "5220/5220 [==============================] - 4s 702us/step - loss: 0.5527 - accuracy: 0.3690 - val_loss: 1.7359 - val_accuracy: 0.4259\n",
      "Epoch 25/1000\n",
      "5220/5220 [==============================] - 4s 708us/step - loss: 0.5518 - accuracy: 0.3621 - val_loss: 1.7816 - val_accuracy: 0.3810\n",
      "Epoch 26/1000\n",
      "5220/5220 [==============================] - 4s 714us/step - loss: 0.5488 - accuracy: 0.3640 - val_loss: 1.7391 - val_accuracy: 0.4241\n",
      "Epoch 27/1000\n",
      "5220/5220 [==============================] - 4s 714us/step - loss: 0.5473 - accuracy: 0.3594 - val_loss: 1.7573 - val_accuracy: 0.4121\n",
      "Epoch 28/1000\n",
      "5220/5220 [==============================] - 4s 701us/step - loss: 0.5437 - accuracy: 0.3573 - val_loss: 1.7530 - val_accuracy: 0.4017\n",
      "Epoch 29/1000\n",
      "5220/5220 [==============================] - 4s 713us/step - loss: 0.5374 - accuracy: 0.3642 - val_loss: 1.7642 - val_accuracy: 0.3879\n",
      "Epoch 30/1000\n",
      "5220/5220 [==============================] - 4s 718us/step - loss: 0.5408 - accuracy: 0.3575 - val_loss: 1.7635 - val_accuracy: 0.3966\n",
      "Epoch 31/1000\n",
      "5220/5220 [==============================] - 4s 719us/step - loss: 0.5371 - accuracy: 0.3529 - val_loss: 1.7613 - val_accuracy: 0.4017\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(W_train_samples, W_train_labels, shuffle = True, batch_size = 1, epochs = 1000, validation_split = 0.1, callbacks = [checkpoint_cb, early_stopping_cb2, tensorboard_cb], class_weight = class_weight)\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
