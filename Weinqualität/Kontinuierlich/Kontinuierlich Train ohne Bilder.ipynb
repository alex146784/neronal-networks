{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren des neuronalen Netzes Kontinuierlich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Beginn müssen wieder alle Bibliotheken eingebunden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary packages\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from numpy import genfromtxt\n",
    "#from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython\n",
    "import kerastuner as kt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Fall, dass die bereits mit Skript 1 vorbereiteten Datensätze nicht im selben Verzeichnis wie die Python-Datei liegt muss das Verzeichnis des Datensatzes angegeben werden. Dies kann durch folgenden Befehl durchgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to set the start working directory manually\n",
    "#not necessary, if the .py file is executed in the directory of the .csv files\n",
    "#os.chdir('D:\\\\OneDrive - bwedu\\\\Uni\\\\09 ABC 1\\\\Neuronale Netzwerke\\\\Python\\\\Wein')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für komplexere Datensätze oder variierte Trainingsparameter kann die Einbindung der GPU von Vorteil sein (für dieses Beispiel nicht benötigt). \n",
    "Die GPU kann ggf. durch folgende Codezeilen eingebunden werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation of the GPU, for this model not necessary\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(\"Num GPUs Available: \", len(physical_devices))\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird das Arbeitsverzeichnis so geändert, dass auf die zuvor vorbereiteten Daten zugegriffen werden kann. \n",
    "\n",
    "Codeabsatz 1 speichert das aktuelle Arbeitsverzeichnis.\n",
    "\n",
    "Codeabsatz 2 ändert das Arbeitsverzeichnis.\n",
    "\n",
    "Codeabsatz 3 importiert die mittels Datenvorbereitungsskript zuvor erstellten Daten.\n",
    "\n",
    "Codeabsatz 4 convertiert die Daten in ein numpy-Array uns skaliert die daten in den Bereich von 0-1. \n",
    "\n",
    "Dabei ist zu beachten, dass die erste Spalte des Datensatzes nicht von Relevanz ist und daher nicht beachtet wird (erste Spalte dient nur zur Nummerierung).\n",
    "\n",
    "Die Daten werden skaliert, indem von den Daten zuert 3 subtrahiert wird, und anschließend mit 6 dividiert wird (z.B. subtrahiere 3 von der Qualität 3-9 = Qualität 0-6, dividiere Qualität 0-6 mit 6 =0-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the current working directory\t \n",
    "current_wd = os.getcwd()\n",
    "\n",
    "#change the working directory to the 'prep_dataset' directroy, where the prepared data are saved as .csv files\n",
    "os.chdir('prep_dataset')\n",
    "\n",
    "#import the prepared data as panda dataframes\n",
    "W_train_samples = pd.read_csv('wine_train_samples.csv')\n",
    "W_train_labels = pd.read_csv('wine_train_labels_cont.csv')\n",
    "\n",
    "#convert panda dataframe to a numpy array\n",
    "#leave out the first column, because this are the rownumernumbers\n",
    "W_train_samples = pd.DataFrame.to_numpy(W_train_samples)[:,1:13]\n",
    "W_train_labels = (pd.DataFrame.to_numpy(W_train_labels)[:,1]-3)/6 #scale the labels in a range form 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die Daten nun importiert wurden, kann wieder zum ursprünglichen Arbeitsverzeichnis gewechselt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the working directory back to the original working directory\n",
    "os.chdir(current_wd)\t \n",
    "\t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Codezeilen erzeugen ein zufälliges neuronales Netz (model_builder), welches später trainiert werden kann. \n",
    "\n",
    "Zu beachten ist, dass dem model_builder die Parameter (Anzahl layer, learning Rate und Anzahl Knoten) systematisch auswählt.\n",
    "\n",
    "Für dieses Beispiel kann der model_builder die Parameter in folgenden Bereichen wählen:\n",
    "\n",
    "Anzahl hidden layer: 1-5\n",
    "\n",
    "Learning Rate: 1e-5 bis 1e-2\n",
    "\n",
    "Knoten der hidden Layer: 16-160 (Differenz der Anzahl an Knoten zwischen den einzelnen Layer: 16 Knoten, um Zeit zu sparen)\n",
    "\n",
    "Aktivierungsfunktion: relu oder sigmoid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "z.B: \n",
    "Modell 1: mit 3 hidden layer, der Lernrate 1e-5 und 16, 32 und 48 Knoten jeweils mit der Aktivierungsfunktion relu\n",
    "Modell 2: mit 2 hidden layer, der Lernrate 1e-3 und 64, 80 Knoten jeweils mit der Aktivierungsfunktion relu\n",
    "Modell 3: mit 5 hidden layer, der Lernrate 1e-2 und 96, 112, 128, 144 und 160 Knoten jeweils mit der Aktivierungsfunktion relu\n",
    "...\n",
    "\n",
    "x repräsentiert jeweils das vorherige Modell \n",
    "\n",
    "\n",
    "Beim kompilieren des Modells ist zu beachten, dass hier als Fehler der mean square error berechnet wird. \n",
    "(Der mittlere quadratische Fehler (Mean Squared Error, MSE) eines Schätzers misst den Durchschnitt der Fehlerquadrate, d.h. die mittlere quadratische Differenz zwischen den geschätzten Werten und dem wahren Wert.) \n",
    "\n",
    "\n",
    "Zum Schluss wird das fertige Modell zurückgegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the definition of the model building function\n",
    "#here it is possible to specify the model, the most parameters\n",
    "#with hp funcitions it is possible to specify a range of values, in which an optimizer can optimize the model\n",
    "def model_builder(hp):\n",
    "\thp_layers = hp.Int('layers', min_value = 1, max_value = 5)#the number of layers in a range from 1 to 5\n",
    "\thp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4, 1e-5]) #test different learning rates \n",
    "\t\n",
    "\t#the input layer with 12 nodes\n",
    "\tinputs = keras.Input(shape=(12,))\n",
    "\t\n",
    "\tx = inputs\n",
    "\tfor i in range(hp_layers):\n",
    "\t\t#for each layer a different number of nodes (16 to 160)\n",
    "\t\tx = keras.layers.Dense(units = hp.Int('units_' + str(i), min_value = 16, max_value = 160, step = 16), activation = hp.Choice('act_func_' + str(i), values = [\"relu\", \"sigmoid\"]))(x)\n",
    "\t\n",
    "\t#the output layer with 1 node (continous regression)\n",
    "\toutputs = keras.layers.Dense(1, activation = hp.Choice('act_func_output', values = [\"relu\", \"sigmoid\"]))(x)\n",
    "\t\n",
    "\t#set the model together\n",
    "\tmodel = keras.Model(inputs, outputs)\n",
    "\t\n",
    "\t#compile the model\n",
    "\tmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate), \n",
    "\t\t\t\t\tloss = \"mean_squared_error\",\n",
    "\t\t\t\t\tmetrics = ['mse'])\n",
    "\t\n",
    "\treturn model #return the finished model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden die einzelnen Einstellungen für den tuner definiert. \n",
    "\n",
    "Wichtig: Sollte eine erneute Optimierung erfolgen, so muss vor jeder Wiederholung zuerst das \n",
    "Verzeichnis des vorherigen tunings manuell gelöscht werden.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project tuner_wine_continuous_actfunc\\wine_quality_continuous\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from tuner_wine_continuous_actfunc\\wine_quality_continuous\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "#set settings for the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_mse', \n",
    "                     max_epochs = 200,\n",
    "                     factor = 5, #the higher, the faster the optimizing, but the smalles the probability to find the best model\n",
    "\t\t\t\t\t\t\t#!!!delete directory before optimizing again!!!\n",
    "                     directory = 'tuner_wine_continuous_actfunc',\n",
    "                     project_name = 'wine_quality_continuous')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Code verwenden wir sogenannte Callback-Funktionen. \n",
    "\n",
    "Sobald über 5 Epochen (patience) kein Fortschritt (Verbesserung der Genauigkeit) mehr erreicht wird, wird das Training abgebrochen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the necessary callbacks \n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden die erzeugten neuronalen Netze optimiert. \n",
    "Folgende Parameter werden dafür verwendet:\n",
    "    \n",
    "    epochs: 200 -> Insgesamt maximal 200 Wiederholungen\n",
    "    batch_size: 6 -> Paralleses Training des neuronalen Netzes durch Daten\n",
    "    validation_split: 0.1 -> 10% der Daten werden für eine Validation verwendet\n",
    "    callbacks (Abruf wie oben definiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#start the optimizer \n",
    "tuner.search(W_train_samples, W_train_labels, epochs = 200, batch_size = 6, validation_split = 0.1, callbacks = [early_stopping_cb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden Befehl wird aus allen getesteten Parametersätzen der beste Parametersatz ausgewählt und gespeichert. \n",
    "Alle anderen neuronalen Netze werden nicht beachtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden print-Befehl werden die vorher definierten Parameter des besten neuronalen Netzes am Bildschirm ausgegeben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tThe hyperparameter search is complete.\n",
      "\t\tLearning rate: 0.001\n",
      "\t\tNumber of Layers: 3\n",
      "\t\t\n",
      "Number of nodes in layer 0: 112.000000\n",
      "Activationfunction in layer 0:relu\n",
      "Number of nodes in layer 1: 16.000000\n",
      "Activationfunction in layer 1:relu\n",
      "Number of nodes in layer 2: 96.000000\n",
      "Activationfunction in layer 2:sigmoid\n",
      "Activationfunction in outputlayersigmoid\n"
     ]
    }
   ],
   "source": [
    "#print the results of the optimizer\n",
    "print(f\"\"\"\n",
    "\t\tThe hyperparameter search is complete.\n",
    "\t\tLearning rate: {best_hps.get('learning_rate')}\n",
    "\t\tNumber of Layers: {best_hps.get('layers')}\n",
    "\t\t\"\"\")\n",
    "for i in range(best_hps.get('layers')):\n",
    "\tprint(\"Number of nodes in layer %i: %f\" %(i, best_hps.get('units_' + str(i))))\n",
    "\tprint(\"Activationfunction in layer %i:\" %(i) + best_hps.get('act_func_' + str(i)) )\n",
    "print(\"Activationfunction in outputlayer\" + best_hps.get('act_func_output') )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch folgenden Befehl kann der Fortschritt graphisch in tensorboard dargestellt werden\n",
    "(um die Daten graphisch darstellen zu können muss zuerst ein log-File erstellt werden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the log-directory for the logfiles for tensorboard\n",
    "root_logdir = os.path.join(os.curdir, \"tensorboard_wine_quality_continuous\")\n",
    "def get_run_logdir():\n",
    "\timport time\n",
    "\trun_id = 'wine_quality_continuous_' + time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "\treturn os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die callbacks werden wie folgt implementiert:\n",
    "\n",
    "Codezeile 1: Speichern der Resultate in einem logfile für Tensorboard.\n",
    "\n",
    "Codezeile 2: Das neuronale Netz speichert immer nur das Modell mit der höchsten Genauigkeit (als .h5-Datei).\n",
    "\n",
    "Codezeile 3 Das neuronale Netz stoppt das Training, wenn nach 10 Wiederholungen keine weitere Erhöhung der Genauigkeit stattfindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the necessary callbacks\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"wine_quality_continious_actfunc.h5\", save_best_only=True)\n",
    "early_stopping_cb2 = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In folgendem Schritt wird das optimale Modell der Variable model zugewiesen, die nun das optimale untrainierte neuronale Netz repräsentiert, das anschließend noch trainiert werden muss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit folgendem Befehl erfolgt das finale Training des neuronalen Netzes.\n",
    "Mit der batch_size 6, maximal 1000 Epochen, 10% der Daten als Validierung und der callback-Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  1/870 [..............................] - ETA: 0s - loss: 0.0092 - mse: 0.0092WARNING:tensorflow:From C:\\Users\\Tobia\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0320s). Check your callbacks.\n",
      "870/870 [==============================] - 1s 1ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0145 - val_mse: 0.0145\n",
      "Epoch 2/1000\n",
      "870/870 [==============================] - 1s 818us/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 3/1000\n",
      "870/870 [==============================] - 1s 956us/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 4/1000\n",
      "870/870 [==============================] - 1s 935us/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 5/1000\n",
      "870/870 [==============================] - 1s 901us/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 6/1000\n",
      "870/870 [==============================] - 1s 894us/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 7/1000\n",
      "870/870 [==============================] - 1s 798us/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 8/1000\n",
      "870/870 [==============================] - 1s 801us/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 9/1000\n",
      "870/870 [==============================] - 1s 785us/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0163 - val_mse: 0.0163\n",
      "Epoch 10/1000\n",
      "870/870 [==============================] - 1s 801us/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 11/1000\n",
      "870/870 [==============================] - 1s 823us/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 12/1000\n",
      "870/870 [==============================] - 1s 851us/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 13/1000\n",
      "870/870 [==============================] - 1s 873us/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 14/1000\n",
      "870/870 [==============================] - 1s 920us/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 15/1000\n",
      "870/870 [==============================] - 1s 843us/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 16/1000\n",
      "870/870 [==============================] - 1s 813us/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 17/1000\n",
      "870/870 [==============================] - 1s 818us/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 18/1000\n",
      "870/870 [==============================] - 1s 797us/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 19/1000\n",
      "870/870 [==============================] - 1s 810us/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 20/1000\n",
      "870/870 [==============================] - 1s 801us/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 21/1000\n",
      "870/870 [==============================] - 1s 821us/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 22/1000\n",
      "870/870 [==============================] - 1s 842us/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 23/1000\n",
      "870/870 [==============================] - 1s 807us/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 24/1000\n",
      "870/870 [==============================] - 1s 820us/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 25/1000\n",
      "870/870 [==============================] - 1s 805us/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 26/1000\n",
      "870/870 [==============================] - 1s 800us/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 27/1000\n",
      "870/870 [==============================] - 1s 814us/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 28/1000\n",
      "870/870 [==============================] - 1s 810us/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 29/1000\n",
      "870/870 [==============================] - 1s 819us/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 30/1000\n",
      "870/870 [==============================] - 1s 806us/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 31/1000\n",
      "870/870 [==============================] - 1s 803us/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 32/1000\n",
      "870/870 [==============================] - 1s 809us/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0121 - val_mse: 0.0121\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(W_train_samples, W_train_labels, batch_size = 6, epochs = 1000, validation_split = 0.1, callbacks = [checkpoint_cb, early_stopping_cb2, tensorboard_cb])\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
